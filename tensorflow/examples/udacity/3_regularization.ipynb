{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "plt.rcParams['figure.figsize'] = 16, 6\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First add regularization to the logistic regression example. Based on example from: https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/mnist/convolutional.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weights))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run logistic regression with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.549574\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 500: 2.460678\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 1000: 1.412287\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 1500: 1.492358\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 2000: 0.988550\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 2500: 1.109251\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 3000: 0.985119\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.0%\n",
      "Test accuracy: 88.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a slight improvement from using regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add regularization to a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 1e-5\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layer_dim = 1024\n",
    "\n",
    "def simple_nn(x, weights_1, biases_1, weights_2, biases_2):\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(x, weights_1) + biases_1)\n",
    "    logits = tf.matmul(hidden_layer, weights_2) + biases_2\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.304495\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 37.7%\n",
      "Minibatch loss at step 500: 0.481019\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1000: 0.425945\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1500: 0.580448\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2000: 0.271807\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2500: 0.506337\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 3000: 0.369680\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 3500: 0.306862\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4000: 0.215989\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Test accuracy: 94.7%\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    weights_1 = weight_variable([image_size * image_size, hidden_layer_dim])\n",
    "    biases_1 = bias_variable([hidden_layer_dim])\n",
    "\n",
    "    weights_2 = weight_variable([hidden_layer_dim, num_labels])\n",
    "    biases_2 = bias_variable([num_labels])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = simple_nn(tf_train_dataset, weights_1, biases_1, weights_2, biases_2)\n",
    "\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))\n",
    "    # Add the regularization term to the loss.\n",
    "    loss += beta * regularizers\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(simple_nn(tf_valid_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "\n",
    "    test_prediction = tf.nn.softmax(simple_nn(tf_test_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "    \n",
    "\n",
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.298821\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 41.3%\n",
      "Minibatch loss at step 500: 0.476666\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.4%\n",
      "Test accuracy: 91.7%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_dim = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    weights_1 = weight_variable([image_size * image_size, hidden_layer_dim])\n",
    "    biases_1 = bias_variable([hidden_layer_dim])\n",
    "\n",
    "    weights_2 = weight_variable([hidden_layer_dim, num_labels])\n",
    "    biases_2 = bias_variable([num_labels])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = simple_nn(tf_train_dataset, weights_1, biases_1, weights_2, biases_2)\n",
    "\n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))\n",
    "    \n",
    "    # Loss function. Add the regularization term to the loss.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * regularizers\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(simple_nn(tf_valid_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "\n",
    "    test_prediction = tf.nn.softmax(simple_nn(tf_test_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "    \n",
    "\n",
    "num_steps = 501\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same small number of batches with no regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.304413\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 41.4%\n",
      "Minibatch loss at step 500: 0.467176\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 91.8%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_dim = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    #weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_dim]))\n",
    "    #biases_1 = tf.Variable(tf.zeros([hidden_layer_dim]))\n",
    "    \n",
    "       \n",
    "    #weights_2 = tf.Variable( tf.truncated_normal([hidden_layer_dim, num_labels]))\n",
    "    #biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    weights_1 = weight_variable([image_size * image_size, hidden_layer_dim])\n",
    "    biases_1 = bias_variable([hidden_layer_dim])\n",
    "\n",
    "    weights_2 = weight_variable([hidden_layer_dim, num_labels])\n",
    "    biases_2 = bias_variable([num_labels])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = simple_nn(tf_train_dataset, weights_1, biases_1, weights_2, biases_2)\n",
    "\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "     # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(simple_nn(tf_valid_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "\n",
    "    test_prediction = tf.nn.softmax(simple_nn(tf_test_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "    \n",
    "\n",
    "num_steps = 501\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_dim = 1024\n",
    "num_steps = 30001\n",
    "SEED = 66478\n",
    "\n",
    "def nn_with_dropout(x, weights_1, biases_1, weights_2, biases_2, keep_prob, train=True):\n",
    "    '''\n",
    "    Simple nn with dropout\n",
    "    '''\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(x, weights_1) + biases_1)\n",
    "    # Adding dropout layer\n",
    "    if train:\n",
    "        hidden_layer = tf.nn.dropout(hidden_layer, 0.5, seed=SEED)\n",
    "        \n",
    "    logits = tf.matmul(hidden_layer, weights_2) + biases_2\n",
    "    return logits\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)#for drop out\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    weights_1 = weight_variable([image_size * image_size, hidden_layer_dim])\n",
    "    biases_1 = bias_variable([hidden_layer_dim])\n",
    "\n",
    "    weights_2 = weight_variable([hidden_layer_dim, num_labels])\n",
    "    biases_2 = bias_variable([num_labels])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = nn_with_dropout(tf_train_dataset, weights_1, biases_1, weights_2, biases_2, keep_prob)\n",
    "\n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))\n",
    "    \n",
    "    # Loss function. Add the regularization term to the loss.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * regularizers\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(nn_with_dropout(tf_valid_dataset, weights_1, biases_1, weights_2, biases_2, None, False))\n",
    "    test_prediction = tf.nn.softmax(nn_with_dropout(tf_test_dataset, weights_1, biases_1, weights_2, biases_2, None, False))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.307951\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 41.5%\n",
      "Minibatch loss at step 500: 0.574442\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 1000: 0.490589\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 1500: 0.707773\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2000: 0.400853\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2500: 0.710152\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 3000: 0.488178\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 3500: 0.444946\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 4000: 0.271859\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 4500: 0.369044\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 5000: 0.446147\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 5500: 0.525655\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 6000: 0.337321\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 6500: 0.345734\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 7000: 0.293261\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7500: 0.409342\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 8000: 0.343128\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8500: 0.216035\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 9000: 0.346171\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 9500: 0.480191\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 10000: 0.470333\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 10500: 0.497426\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 11000: 0.436880\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 11500: 0.327063\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 12000: 0.530959\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 12500: 0.437585\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 13000: 0.408631\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 13500: 0.595047\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 14000: 0.259729\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 14500: 0.376733\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 15000: 0.462555\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 15500: 0.313214\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 16000: 0.442843\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 16500: 0.380057\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 17000: 0.295323\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 17500: 0.411688\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 18000: 0.103358\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 18500: 0.270819\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 19000: 0.251635\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 19500: 0.332070\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 20000: 0.383087\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 20500: 0.281084\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 21000: 0.403339\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 21500: 0.371669\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 22000: 0.448122\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 22500: 0.267666\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 23000: 0.393800\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 23500: 0.321455\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 24000: 0.423278\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 24500: 0.374818\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 25000: 0.329573\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 25500: 0.341574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 26000: 0.330858\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 26500: 0.245712\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 27000: 0.279313\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 27500: 0.282960\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 28000: 0.414634\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 28500: 0.440346\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 29000: 0.279982\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 29500: 0.350318\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 30000: 0.307039\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.7%\n",
      "Test accuracy: 95.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/pymodules/python2.7/matplotlib/legend.py:317: UserWarning: Unrecognized location \"validate\". Falling back on \"best\"; valid locations are\n",
      "\tright\n",
      "\tcenter left\n",
      "\tupper right\n",
      "\tlower right\n",
      "\tbest\n",
      "\tcenter\n",
      "\tlower left\n",
      "\tcenter right\n",
      "\tupper left\n",
      "\tupper center\n",
      "\tlower center\n",
      "\n",
      "  % (loc, '\\n\\t'.join(self.codes.iterkeys())))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIiIiIiIiIiIiIiIiIE/4/V2Lrkp7FrF8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61f0524b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_accuracy_step = 500\n",
    "with tf.Session(graph=graph) as session:\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            train_loss.append(accuracy(predictions, batch_labels))\n",
    "            validation_loss.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_loss[-1])\n",
    "            print(\"Validation accuracy: %.1f%%\" % validation_loss[-1])\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    # Plot learning curve\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(train_loss))*sample_accuracy_step, train_loss, label=\"train\")\n",
    "    plt.plot(np.arange(len(train_loss))*sample_accuracy_step, validation_loss, label=\"validate\")\n",
    "    plt.legend(\"train\", \"validate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_spec = np.array([1024, 300])\n",
    "num_of_hidden_layers = hidden_layer_spec.shape[0]\n",
    "\n",
    "def nn_with_dropout(x, weights, biases, keep_probs, train=True):\n",
    "    '''\n",
    "    Deep nn with dropout\n",
    "    '''\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(x, weights[0]) + biases[0])\n",
    "    # Adding dropout layer\n",
    "    if train:\n",
    "        hidden_layer = tf.nn.dropout(hidden_layer, 0.5, seed=SEED)\n",
    "        \n",
    "    logits = tf.matmul(hidden_layer, weights[1]) + biases[1]\n",
    "    return logits"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
