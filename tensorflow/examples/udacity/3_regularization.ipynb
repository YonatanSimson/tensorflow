{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "plt.rcParams['figure.figsize'] = 16, 6\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First add regularization to the logistic regression example. Based on example from: https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/mnist/convolutional.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weights))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run logistic regression with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.549574\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 500: 2.460678\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 1000: 1.412287\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 1500: 1.492358\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 2000: 0.988550\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 2500: 1.109251\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 3000: 0.985119\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.0%\n",
      "Test accuracy: 88.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a slight improvement from using regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add regularization to a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 1e-5\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layer_dim = 1024\n",
    "\n",
    "def simple_nn(x, weights_1, biases_1, weights_2, biases_2):\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(x, weights_1) + biases_1)\n",
    "    logits = tf.matmul(hidden_layer, weights_2) + biases_2\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.304495\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 37.7%\n",
      "Minibatch loss at step 500: 0.481019\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1000: 0.425945\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1500: 0.580448\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2000: 0.271807\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2500: 0.506337\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 3000: 0.369680\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 3500: 0.306862\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4000: 0.215989\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Test accuracy: 94.7%\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    weights_1 = weight_variable([image_size * image_size, hidden_layer_dim])\n",
    "    biases_1 = bias_variable([hidden_layer_dim])\n",
    "\n",
    "    weights_2 = weight_variable([hidden_layer_dim, num_labels])\n",
    "    biases_2 = bias_variable([num_labels])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = simple_nn(tf_train_dataset, weights_1, biases_1, weights_2, biases_2)\n",
    "\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))\n",
    "    # Add the regularization term to the loss.\n",
    "    loss += beta * regularizers\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(simple_nn(tf_valid_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "\n",
    "    test_prediction = tf.nn.softmax(simple_nn(tf_test_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "    \n",
    "\n",
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.298821\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 41.3%\n",
      "Minibatch loss at step 500: 0.476666\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.4%\n",
      "Test accuracy: 91.7%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_dim = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    weights_1 = weight_variable([image_size * image_size, hidden_layer_dim])\n",
    "    biases_1 = bias_variable([hidden_layer_dim])\n",
    "\n",
    "    weights_2 = weight_variable([hidden_layer_dim, num_labels])\n",
    "    biases_2 = bias_variable([num_labels])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = simple_nn(tf_train_dataset, weights_1, biases_1, weights_2, biases_2)\n",
    "\n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))\n",
    "    \n",
    "    # Loss function. Add the regularization term to the loss.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * regularizers\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(simple_nn(tf_valid_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "\n",
    "    test_prediction = tf.nn.softmax(simple_nn(tf_test_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "    \n",
    "\n",
    "num_steps = 501\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same small number of batches with no regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.304413\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 41.4%\n",
      "Minibatch loss at step 500: 0.467176\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 91.8%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_dim = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    #weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_dim]))\n",
    "    #biases_1 = tf.Variable(tf.zeros([hidden_layer_dim]))\n",
    "    \n",
    "       \n",
    "    #weights_2 = tf.Variable( tf.truncated_normal([hidden_layer_dim, num_labels]))\n",
    "    #biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    weights_1 = weight_variable([image_size * image_size, hidden_layer_dim])\n",
    "    biases_1 = bias_variable([hidden_layer_dim])\n",
    "\n",
    "    weights_2 = weight_variable([hidden_layer_dim, num_labels])\n",
    "    biases_2 = bias_variable([num_labels])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = simple_nn(tf_train_dataset, weights_1, biases_1, weights_2, biases_2)\n",
    "\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "     # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(simple_nn(tf_valid_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "\n",
    "    test_prediction = tf.nn.softmax(simple_nn(tf_test_dataset, weights_1, biases_1, weights_2, biases_2))\n",
    "    \n",
    "\n",
    "num_steps = 501\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_dim = 1024\n",
    "num_steps = 30001\n",
    "SEED = 66478\n",
    "\n",
    "def nn_with_dropout(x, weights_1, biases_1, weights_2, biases_2, keep_prob, train=True):\n",
    "    '''\n",
    "    Simple nn with dropout\n",
    "    '''\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(x, weights_1) + biases_1)\n",
    "    # Adding dropout layer\n",
    "    if train:\n",
    "        hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "        \n",
    "    logits = tf.matmul(hidden_layer, weights_2) + biases_2\n",
    "    return logits\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)#for drop out\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    weights_1 = weight_variable([image_size * image_size, hidden_layer_dim])\n",
    "    biases_1 = bias_variable([hidden_layer_dim])\n",
    "\n",
    "    weights_2 = weight_variable([hidden_layer_dim, num_labels])\n",
    "    biases_2 = bias_variable([num_labels])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = nn_with_dropout(tf_train_dataset, weights_1, biases_1, weights_2, biases_2, keep_prob)\n",
    "\n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))\n",
    "    \n",
    "    # Loss function. Add the regularization term to the loss.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * regularizers\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(nn_with_dropout(tf_valid_dataset, weights_1, biases_1, weights_2, biases_2, None, False))\n",
    "    test_prediction = tf.nn.softmax(nn_with_dropout(tf_test_dataset, weights_1, biases_1, weights_2, biases_2, None, False))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.307951\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 41.5%\n",
      "Minibatch loss at step 500: 0.574442\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 1000: 0.490589\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 1500: 0.707773\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2000: 0.400853\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2500: 0.710152\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 3000: 0.488178\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 3500: 0.444946\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 4000: 0.271859\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 4500: 0.369044\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 5000: 0.446147\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 5500: 0.525655\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 6000: 0.337321\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 6500: 0.345734\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 7000: 0.293261\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7500: 0.409342\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 8000: 0.343128\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8500: 0.216035\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 9000: 0.346171\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 9500: 0.480191\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 10000: 0.470333\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 10500: 0.497426\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 11000: 0.436880\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 11500: 0.327063\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 12000: 0.530959\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 12500: 0.437585\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 13000: 0.408631\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 13500: 0.595047\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 14000: 0.259729\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 14500: 0.376733\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 15000: 0.462555\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 15500: 0.313214\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 16000: 0.442843\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 16500: 0.380057\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 17000: 0.295323\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 17500: 0.411688\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 18000: 0.103358\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 18500: 0.270819\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 19000: 0.251635\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 19500: 0.332070\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 20000: 0.383087\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 20500: 0.281084\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 21000: 0.403339\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 21500: 0.371669\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 22000: 0.448122\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 22500: 0.267666\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 23000: 0.393800\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 23500: 0.321455\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 24000: 0.423278\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 24500: 0.374818\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 25000: 0.329573\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 25500: 0.341574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 26000: 0.330858\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 26500: 0.245712\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 27000: 0.279313\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 27500: 0.282960\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 28000: 0.414634\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 28500: 0.440346\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 29000: 0.279982\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 29500: 0.350318\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 30000: 0.307039\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.7%\n",
      "Test accuracy: 95.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/pymodules/python2.7/matplotlib/legend.py:317: UserWarning: Unrecognized location \"validate\". Falling back on \"best\"; valid locations are\n",
      "\tright\n",
      "\tcenter left\n",
      "\tupper right\n",
      "\tlower right\n",
      "\tbest\n",
      "\tcenter\n",
      "\tlower left\n",
      "\tcenter right\n",
      "\tupper left\n",
      "\tupper center\n",
      "\tlower center\n",
      "\n",
      "  % (loc, '\\n\\t'.join(self.codes.iterkeys())))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIiIiIiIiIiIiIiIiIE/4/V2Lrkp7FrF8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61f0524b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_accuracy_step = 500\n",
    "with tf.Session(graph=graph) as session:\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            train_loss.append(accuracy(predictions, batch_labels))\n",
    "            validation_loss.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_loss[-1])\n",
    "            print(\"Validation accuracy: %.1f%%\" % validation_loss[-1])\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    # Plot learning curve\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(train_loss))*sample_accuracy_step, train_loss, label=\"train\")\n",
    "    plt.plot(np.arange(len(train_loss))*sample_accuracy_step, validation_loss, label=\"validate\")\n",
    "    plt.legend(loc='lower center')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.302770\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 500: 0.547579\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 1000: 0.397043\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1500: 0.584772\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2000: 0.419739\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2500: 0.580277\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 3000: 0.510397\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 3500: 0.365076\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4000: 0.245457\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 4500: 0.385564\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 5000: 0.467229\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 5500: 0.523425\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 6000: 0.334866\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 6500: 0.467245\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 7000: 0.254409\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 7500: 0.328690\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 8000: 0.434321\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 8500: 0.203289\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 9000: 0.342222\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9500: 0.422699\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 10000: 0.451580\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Test accuracy: 94.7%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61f0792050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "SEED = 12345\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "num_steps = 10001\n",
    "\n",
    "hidden_layer_dims = np.array([1024, 256])\n",
    "num_of_hidden_layers = hidden_layer_dims.shape[0]\n",
    "\n",
    "def deep_nn_with_dropout(x, weights, biases, train=True):\n",
    "    '''\n",
    "    Deep nn with dropout\n",
    "    '''\n",
    "\n",
    "    hidden_layer = []\n",
    "    # Hidden layer 1\n",
    "    hidden_layer.append(tf.nn.relu(tf.matmul(x, weights[0]) + biases[0]))\n",
    "    # Adding dropout layer\n",
    "    if train:\n",
    "        hidden_layer[-1] = tf.nn.dropout(hidden_layer[-1], 0.5, seed=SEED)\n",
    "\n",
    "    hidden_layer.append(tf.nn.relu(tf.matmul(hidden_layer[-1], weights[1]) + biases[1]))\n",
    "    # Adding dropout layer\n",
    "    if train:\n",
    "        hidden_layer[-1] = tf.nn.dropout(hidden_layer[-1], 0.5, seed=SEED)\n",
    "\n",
    "    logits = tf.matmul(hidden_layer[-1], weights[2]) + biases[2]\n",
    "    return logits\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)#for drop out\n",
    "\n",
    "    dimensions = np.hstack((np.array([n_input]), hidden_layer_dims,  np.array([n_classes])))\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for k in range(num_of_hidden_layers + 1):\n",
    "        weights.append(weight_variable([dimensions[k], dimensions[k+1]]))\n",
    "        biases.append(bias_variable([dimensions[k+1]]))\n",
    "\n",
    "\n",
    "    # Training computation.\n",
    "    logits = deep_nn_with_dropout(tf_train_dataset, weights, biases)\n",
    "\n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = tf.nn.l2_loss(weights[0]) + tf.nn.l2_loss(weights[1]) + tf.nn.l2_loss(weights[2])\n",
    "\n",
    "    # Loss function. Add the regularization term to the loss.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta * regularizers\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(deep_nn_with_dropout(tf_valid_dataset, weights, biases, False))\n",
    "    test_prediction = tf.nn.softmax(deep_nn_with_dropout(tf_test_dataset, weights, biases, False))\n",
    "\n",
    "\n",
    "sample_accuracy_step = 500\n",
    "with tf.Session(graph=graph) as session:\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            train_loss.append(accuracy(predictions, batch_labels))\n",
    "            validation_loss.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_loss[-1])\n",
    "            print(\"Validation accuracy: %.1f%%\" % validation_loss[-1])\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    # Plot learning curve\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(train_loss))*sample_accuracy_step, train_loss, label=\"train\")\n",
    "    plt.plot(np.arange(len(train_loss))*sample_accuracy_step, validation_loss, label=\"validate\")\n",
    "    plt.legend(loc='lower center')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.303259\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 19.7%\n",
      "Minibatch loss at step 500: 0.584231\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 1000: 0.366118\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1500: 0.574664\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2000: 0.353571\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2500: 0.562946\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 3000: 0.448101\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 3500: 0.412349\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4000: 0.308015\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 4500: 0.329415\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 5000: 0.463207\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 5500: 0.441183\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 6000: 0.268444\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 6500: 0.512291\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 7000: 0.299839\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 7500: 0.282158\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 8000: 0.357831\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 8500: 0.174856\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 9000: 0.286720\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 9500: 0.346563\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 10000: 0.398129\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 10500: 0.299128\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 11000: 0.427615\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 11500: 0.370002\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 12000: 0.456618\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 12500: 0.440362\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 13000: 0.388023\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 13500: 0.366531\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 14000: 0.171782\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 14500: 0.303682\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 15000: 0.450755\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.5%\n",
      "Test accuracy: 95.7%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wM6yAf3Ax5SuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f621d522310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate_start = 0.5\n",
    "num_steps = 15001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)#for drop out\n",
    "\n",
    "    dimensions = np.hstack((np.array([n_input]), hidden_layer_dims,  np.array([n_classes])))\n",
    "\n",
    "    #####################\n",
    "    ### first layer #####\n",
    "    #####################\n",
    "    # Variables.\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for k in range(num_of_hidden_layers + 1):\n",
    "        weights.append(weight_variable([dimensions[k], dimensions[k+1]]))\n",
    "        biases.append(bias_variable([dimensions[k+1]]))\n",
    "\n",
    "\n",
    "    # Training computation.\n",
    "    logits = deep_nn_with_dropout(tf_train_dataset, weights, biases)\n",
    "\n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = tf.nn.l2_loss(weights[0]) + tf.nn.l2_loss(weights[1]) + tf.nn.l2_loss(weights[2])\n",
    "\n",
    "    # Loss function. Add the regularization term to the loss.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta * regularizers\n",
    "\n",
    "    # Optimizer with decaying learning rate\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate_start, global_step,1000,0.90,staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(deep_nn_with_dropout(tf_valid_dataset, weights, biases, False))\n",
    "    test_prediction = tf.nn.softmax(deep_nn_with_dropout(tf_test_dataset, weights, biases, False))\n",
    "\n",
    "\n",
    "sample_accuracy_step = 500\n",
    "with tf.Session(graph=graph) as session:\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            train_loss.append(accuracy(predictions, batch_labels))\n",
    "            validation_loss.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_loss[-1])\n",
    "            print(\"Validation accuracy: %.1f%%\" % validation_loss[-1])\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    # Plot learning curve\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(train_loss))*sample_accuracy_step, train_loss, label=\"train\")\n",
    "    plt.plot(np.arange(len(train_loss))*sample_accuracy_step, validation_loss, label=\"validate\")\n",
    "    plt.legend(loc='lower center')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
